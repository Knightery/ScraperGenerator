#!/usr/bin/env python3
"""
MONITOR MODE SCRAPER - Generated by AI Navigator
Company: {company_name}
URL: {scrape_url}
Generated at: {generated_at}

This scraper monitors a page for changes that might indicate new internship listings.
When the page changes significantly, it can trigger the creation of a full scraper.
"""

import hashlib
import logging
import sys
import os

# Add the parent directory to the path to import our modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from supabase_database import SupabaseDatabaseManager
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import time

def setup_logging():
    """Setup logging for the monitor scraper."""
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'logs/{log_filename}'),
            logging.StreamHandler()
        ]
    )

def get_monitor_config():
    """Get the monitor configuration for {{company_name}}."""
    return {{
        'company_name': '{company_name}',
        'scrape_url': '{scrape_url}',
        'search_required': {search_required},
        'search_input_selector': '{search_input_selector}',
        'search_submit_selector': '{search_submit_selector}',
        'search_query': '{search_query}',
    }}

def compute_html_hash(html_content: str) -> str:
    """Compute SHA256 hash of HTML content for change detection."""
    return hashlib.sha256(html_content.encode('utf-8')).hexdigest()

def perform_search_interaction(page, config):
    """Perform search interaction if configured."""
    if not config.get('search_required'):
        return
    
    search_input_selector = config.get('search_input_selector', '')
    search_submit_selector = config.get('search_submit_selector', '')
    search_query = config.get('search_query', '')
    
    is_button_mode = not search_input_selector and not search_query and search_submit_selector
    is_search_mode = search_input_selector and search_query
    
    if is_button_mode:
        logging.info(f"Clicking button: {{search_submit_selector}}")
        page.evaluate(f"document.querySelector({{repr(search_submit_selector)}}).click()")
    elif is_search_mode:
        logging.info(f"Filling search input with: {{search_query}}")
        page.locator(search_input_selector).fill(search_query)
        
        if search_submit_selector:
            logging.info(f"Clicking submit button")
            page.evaluate(f"document.querySelector({{repr(search_submit_selector)}}).click()")
        else:
            logging.info("Pressing Enter to submit")
            page.locator(search_input_selector).press('Enter')
    
    # Wait for results
    try:
        page.wait_for_load_state('networkidle', timeout=15000)
    except PlaywrightTimeoutError:
        logging.warning("Timeout waiting for search results")
    
    time.sleep(5)  # Additional wait for dynamic content

def get_current_page_snapshot(config):
    """Navigate to page, perform search, and capture HTML snapshot."""
    logging.info(f"Capturing page snapshot for monitoring...")
    
    with sync_playwright() as p:
        browser = p.firefox.launch(headless=True)
        context = browser.new_context(
            viewport={{'width': 1920, 'height': 1080}},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0'
        )
        page = context.new_page()
        
        try:
            # Navigate to page
            logging.info(f"Navigating to: {{config['scrape_url']}}")
            try:
                page.goto(config['scrape_url'], wait_until='networkidle', timeout=30000)
            except PlaywrightTimeoutError:
                logging.warning("Timeout loading page, proceeding with available content")
            
            time.sleep(5)  # Wait for dynamic content
            
            # Perform search interaction if configured
            perform_search_interaction(page, config)
            
            # Get HTML content
            html_content = page.content()
            
            # Compute hash
            html_hash = compute_html_hash(html_content)
            
            logging.info(f"Captured snapshot, hash: {{html_hash[:16]}}...")
            
            return {{
                'html_hash': html_hash,
                'html_content': html_content[:10000],  # Store first 10k chars for reference
                'success': True
            }}
            
        except Exception as e:
            logging.error(f"Error capturing snapshot: {{str(e)}}")
            return {{
                'html_hash': None,
                'html_content': None,
                'success': False,
                'error': str(e)
            }}
        finally:
            browser.close()

def main():
    """Main monitor function."""
    setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("Starting {company_name} monitor scraper...")
    
    config = get_monitor_config()
    db_manager = SupabaseDatabaseManager()
    
    # Get company info from database
    company = db_manager.get_company_by_name('{company_name}')
    if not company:
        logger.error("Company '{{company_name}}' not found in database")
        print("Error: Company not found in database. Please add the company first.")
        return
    
    # Capture current page snapshot
    snapshot = get_current_page_snapshot(config)
    
    if not snapshot['success']:
        logger.error(f"Failed to capture snapshot: {{snapshot.get('error')}}")
        db_manager.log_scraper_execution(
            company['id'], 
            0, 
            success=False,
            error_message=f"Monitor failed: {{snapshot.get('error')}}"
        )
        return
    
    current_hash = snapshot['html_hash']
    
    # Get previous snapshot from database
    previous_snapshot = db_manager.get_monitor_snapshot(company['id'])
    
    if not previous_snapshot:
        # First run - store snapshot
        logger.info("First monitoring run - storing baseline snapshot")
        db_manager.save_monitor_snapshot(
            company['id'],
            config['scrape_url'],
            current_hash,
            snapshot['html_content']
        )
        
        print(f"\\n=== MONITOR MODE INITIALIZED ===")
        print(f"Company: '{company_name}'")
        print(f"URL: '{scrape_url}'")
        print(f"Status: No internships currently available")
        print(f"Baseline snapshot saved - run again to check for changes")
        
        db_manager.log_scraper_execution(
            company['id'], 
            0, 
            success=True,
            error_message="Monitor initialized - no internships found"
        )
    else:
        # Compare with previous snapshot
        previous_hash = previous_snapshot.get('html_hash')
        
        if current_hash == previous_hash:
            logger.info("No changes detected - page content is identical")
            print(f"\\n=== NO CHANGES DETECTED ===")
            print(f"Company: '{company_name}'")
            print(f"Status: Still no internships available")
            print(f"Last checked: {{previous_snapshot.get('last_checked')}}")
            
            # Update last checked timestamp
            db_manager.update_monitor_snapshot_timestamp(company['id'])
            
            db_manager.log_scraper_execution(
                company['id'], 
                0, 
                success=True,
                error_message="Monitor check - no changes detected"
            )
        else:
            logger.info("CHANGES DETECTED! Page content has changed significantly")
            print(f"\\n=== ALERT: CHANGES DETECTED ===")
            print(f"Company: '{company_name}'")
            print(f"Status: Page content changed - internships may be available!")
            print(f"")
            print(f"NEXT STEPS:")
            print(f"1. The page content has changed since last check")
            print(f"2. This may indicate new internship listings")
            print(f"3. Run: python scrape_cli.py add \"{{company_name}}\"")
            print(f"4. This will re-analyze the page and create a full scraper if internships are found")
            
            # Update snapshot with new hash
            db_manager.save_monitor_snapshot(
                company['id'],
                config['scrape_url'],
                current_hash,
                snapshot['html_content']
            )
            
            db_manager.log_scraper_execution(
                company['id'], 
                0, 
                success=True,
                error_message="ALERT: Page changes detected - may have new internships"
            )

if __name__ == "__main__":
    main()
